{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Bulgarian text with KerasNLP and transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook follows [this](https://keras.io/examples/generative/text_generation_gpt/) Keras tutorial, but switches the language to Bulgarian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Natural language processing (NLP) has advanced significantly in recent years, driven by the development of large-scale pre-trained recurrent or transformer language models (LMs) such as BERT and GPT. Yet, the NLP research community still faces challenges related to the scarcity of comprehensive and diverse datasets for pre-training Transformer models in less-resourced languages, including Bulgarian.\n",
    "\n",
    "This project uses KerasNLP to build and to train aa scaled down Generative Pre-Trained (GPT) model for Bulgarian text generation. GPT is a Transformer-based model that allows the generation of sophisticated text from a prompt. \n",
    "\n",
    "This example demonstrates how KerasNLP tokenization, layers and metrics simplify the training process, and then show how to generate output text using the KerasNLP sampling utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Transformer for Bulgarian\n",
    "\n",
    "#### Setup dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently working with keras 2.10.0 and Tensorflow 2.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings & hyperparameters\n",
    "\n",
    "Define constants that will be used later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 5\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "\n",
    "The trainig and validation datasets are available [here](https://github.com/radev2711/Learning-Deep-Lerning/tree/main/data).\n",
    "\n",
    "The dataset consists of different themed text documents in Bulgarian collected from Wikipedia.bg, media sites - Investor.bg, DW.bg, Sportal.bg, whit the majority coming from Chitanka's Bulgarian literature category.\n",
    "\n",
    "Currently the Bulgarian training set contains 29_970_431 with 960_944 characters in the validation set compared to the original 407_403_786 characters of the original English training set with 867_476 characters in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train set and filter out short lines with splitting it into baches and shuffeling the elements of the dataset.\n",
    "raw_train_ds = (\n",
    "    tf_data.TextLineDataset(\"data/bg_train.txt\")\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation set and filter out short lines with splitting it into baches.\n",
    "raw_val_ds = (\n",
    "    tf_data.TextLineDataset(\"data/bg_valid.txt\")\n",
    "    .filter(lambda x: tf_strings.length(x) > MIN_STRING_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer\n",
    "\n",
    "Training the tokenizer from the training dataset for a vocabulary size of VOCAB_SIZE, which is a tuned hyperparameter.\n",
    " \n",
    "Limiting the vocabulary as much as possible has a large effect on the number of model parameters, but including too few vocabulary terms, wich can lead to too many out-of-vocabulary (OOV) sub-words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer vocabulary\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    lowercase=True,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary data is used to initialize keras_nlp.tokenizers.WordPieceTokenizer. \n",
    "\n",
    "WordPieceTokenizer is an efficient implementation of the WordPiece algorithm used by BERT and other models. It will strip, lower-case and do other irreversible preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tokenizing and splitting it into features and labels the datasets are preprocesed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packer adds a start token\n",
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Tokenize and split into train and label sequences.\n",
    "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")\n",
    "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf_data.AUTOTUNE).prefetch(\n",
    "    tf_data.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model\n",
    "\n",
    "Create a scaled down GPT model with the following layers:\n",
    "\n",
    "* One keras_nlp.layers.TokenAndPositionEmbedding layer, which combines the embedding for the token and its position.\n",
    "* Multiple keras_nlp.layers.TransformerDecoder layers, with the default causal masking. The layer has no cross-attention when run with decoder sequence only.\n",
    "* One final dense linear layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "# Transformer decoders.\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)  # Giving one argument only skips cross-attention.\n",
    "# Output.\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our model summary. A large majority of the parameters are in the token_and_position_embedding and the output dense layer. This means that the vocabulary size (VOCAB_SIZE) has a large effect on the size of the model, while the number of Transformer decoder layers (NUM_LAYERS) doesn't affect it as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, None, 256)        1312768   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, None, 256)        329085    \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tran  (None, None, 256)        329085    \n",
      " sformerDecoder)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,255,938\n",
      "Trainable params: 3,255,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Now that we have our model, let's train it with the fit() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP perplexity measures how likely the model is to generate the input text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "495/495 [==============================] - 266s 530ms/step - loss: 5.2432 - perplexity: 273.5323 - val_loss: 4.6678 - val_perplexity: 145.7430\n",
      "Epoch 2/5\n",
      "495/495 [==============================] - 272s 546ms/step - loss: 4.4183 - perplexity: 113.3868 - val_loss: 4.3095 - val_perplexity: 99.4327\n",
      "Epoch 3/5\n",
      "495/495 [==============================] - 330s 663ms/step - loss: 4.1396 - perplexity: 84.1064 - val_loss: 4.1534 - val_perplexity: 84.1033\n",
      "Epoch 4/5\n",
      "495/495 [==============================] - 273s 549ms/step - loss: 3.9870 - perplexity: 71.4102 - val_loss: 4.0842 - val_perplexity: 78.3081\n",
      "Epoch 5/5\n",
      "495/495 [==============================] - 271s 544ms/step - loss: 3.8828 - perplexity: 63.8699 - val_loss: 4.0653 - val_perplexity: 76.7267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a02a64d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and loading the model\n",
    "\n",
    "Tensorflow and Keras allows for model progress to be saved during and after training. This means a model can resume where it left off and avoid long training times or be easily shared or published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model/model_name.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.keras.models.load_model('model/model_name.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "With the model trained, it is time to test it out to gauge its performance. In order to do this the model needs a seed with an input sequence starting with the \"[BOS]\" token, followed by progressively sampling the model by making predictions for each subsequent token in a loop.\n",
    "\n",
    "To start lets build a prompt with the same shape as our model inputs, containing only the \"[BOS]\" token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"packer\" layers adds the [BOS] token for us.\n",
    "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inference the keras_nlp.samplers module can be used. The module  requires a callback function wrapping the model we just trained. This wrapper calls the model and returns the logit predictions for the current token we are generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    # Ignore hidden states for now; only needed for contrastive search.\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the wrapper function is the most complex part of using these functions. With that done, let's test out the different utilities, starting with greedy search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedy search\n",
    "\n",
    "This methos greedily picks the most probable token at each timestep. In other words, we get the argmax of the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search generated text: \n",
      "[BOS] — не е ли ? — каза той , — каза той . — не е ли ? — не е ли ? — каза той . — не е ли ? — каза той . — не е ли ? — каза той . — не е ли ? — не ! — отвърна той . — не е ли ? — не ! — отвърна той . — не е ли ? — не ! — отвърна той . — не е ли ? — не ! — отвърна той . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.GreedySampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,  # Start sampling immediately after the [BOS] token.\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "txt = txt.numpy()[0].decode('utf-8') #decode the output bytes of the model into cyrillic characters\n",
    "print(f\"Greedy search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy search starts out making some sense, but quickly starts repeating itself. This is a common problem with text generation that can be fixed by some of the probabilistic text generation utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Beam search\n",
    "\n",
    "At a high-level, beam search keeps track of the num_beams most probable sequences at each timestep, and predicts the best next token from all sequences. It is an improvement over greedy search since it stores more possibilities. However, it is less efficient than greedy search since it has to compute and store multiple potential sequences.\n",
    "\n",
    "Note: beam search with num_beams=1 is identical to greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generated text: \n",
      "[BOS] вълчицата продължаваше да се съобразява , че се е случвало да се съобразява , че се е случвало да се съобразява , че се е случвало да се съобразява , че се е случвало да се осъществява и да се съобразява , че трябва да се съобразява , че трябва да се осъществява и да се съобрази . . . . . . . [PAD] ! [PAD] ! [PAD] ! . . . . . [PAD] ! [PAD] ! . . . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "txt = txt.numpy()[0].decode('utf-8')\n",
    "print(f\"Beam search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to greedy search, beam search quickly starts repeating itself, since it is still a deterministic method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search\n",
    "\n",
    "Random search is a probabilistic method. At each time step, it samples the next token using the softmax probabilities provided by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search generated text: \n",
      "[BOS] — исках да узнаете тази европа , със знанието ми е нужен . даия аз наистина ви казах и трудно мога да се изляза от мен . седим ли на алиомана поне затуй пък ти го направих толкова инициаторителни едновременно подеживни . трябва да ми експертите и да ти спре . . тръгна ли прекалено близкото вътре , в черноокия тя . бива ли е наистина обикновена земя , а моята смешнашко глътка апепеля . [PAD]х я гледах , вина — приятелката дойде на тия , дето все не бях ще се с нас\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !!! Sometimes breaks the kernel. !!!\n",
    "\n",
    "sampler = keras_nlp.samplers.RandomSampler()\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "txt = txt.numpy()[0].decode('utf-8')\n",
    "print(f\"Random search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random approach elimienates repetitions, but may result in some nonsensical words appearing since any word in the vocabulary has a chance of appearing with this sampling method. This is fixed by the next search utility, top-k search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top-K search\n",
    "\n",
    "Similar to random search, we sample the next token from the probability distribution provided by the model. The only difference is that here, we select out the top k most probable tokens, and distribute the probability mass over them before sampling. This way, we won't be sampling from low probability tokens, and hence we would have less nonsensical words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-K search generated text: \n",
      "[BOS] андро се разсладиха войнства на гьонг се чувствуваше , че и той се преизъртили в килим , за да замине на косъм . и когато стреля там с водопроводи , въоди селищата на коприваци , и които бяха разкъсали и изминални , а валутни , които се радваха . той се разшириха , а се изпълнява , че стреля , водородопанските им ор\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "txt = txt.numpy()[0].decode('utf-8')\n",
    "print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top-P search\n",
    "\n",
    "Even with the top-k search, there is something to improve upon. With top-k search, the number k is fixed, which means it selects the same number of tokens for any probability distribution.\n",
    "\n",
    "Lets consider two scenarios, one where the probability mass is concentrated over 2 words and another where the probability mass is evenly concentrated across 10. Should we choose k=2 or k=10? There is no one size that fits all k here.\n",
    "\n",
    "This is where top-p search comes in. Instead of choosing a k, we choose a probability p that we want the probabilities of the top tokens to sum up to. This way, we can dynamically adjust the k based on the probability distribution. By setting p=0.9, if 90% of the probability mass is concentrated on the top 2 tokens, we can filter out the top 2 tokens to sample from. If instead the 90% is distributed over 10 tokens, it will similarly filter out the top 10 tokens to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-P search generated text: \n",
      "[BOS] когато стана , че елементаре ! . . . . но сега , когато се вървеят не може да не се промени да изобилие . да се усмихне , и той се припомни от всички тия думи , които се ухили , и началниците на свободата да се утвърди , че ще го успокояват с тия страшни очи . . [PAD]ът ще го вземе , че е открай ? . . . [PAD] не може да стане , защото те не е безсилно , но все пак не се е случило . [PAD] да стане , да\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt=prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "txt = txt.numpy()[0].decode('utf-8')\n",
    "print(f\"Top-P search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using callbacks for text generation\n",
    "\n",
    "These utilities can alos be wrapped in a callback, which allows the printing out of a prediction sequence for every epoch of the model. Here is an example of a callback for top-k search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Top-K search generated text: \n",
      "[BOS] — виж , че не си поприказват , защото няма . . не знам какво да правя . аз ще се повъртат , ако не си говоря за да разбере дали няма да си представим , а ти , а може би . . . [PAD] ! аз се опитвам да си представя как ще я навреме с теб и аз ще се свърме ! [PAD] ! [PAD] ! и като че ли ще побереш ли е ? [PAD] ! ще ти отида ! [PAD] ! — отвързаш ! и аз , като да не знам какво правиш с тебе ! и знаеш !\n",
      "\n",
      "1/1 - 5s - loss: 3.8140 - perplexity: 67.5890 - 5s/epoch - 5s/step\n",
      "Epoch 2/2\n",
      "Top-K search generated text: \n",
      "[BOS] когато засущните османогнизираха се вълнуваха . той се чувствуваше как да се събират в корита . не се изгоряло от бунтовникът се разсеянова на магията в инстирумент . страдаците не можеха да направят , а несъмнено от разтърсимост от коператичната кооприна . [PAD] гишето на постройка . по - малко се бяха врушени стъкналите на благодарности , ск\n",
      "\n",
      "1/1 - 5s - loss: 3.6728 - perplexity: 61.7761 - 5s/epoch - 5s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a0a18460>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=next,\n",
    "            prompt=prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        txt = txt.numpy()[0].decode('utf-8')\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
    "\n",
    "\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "# Dummy training loop to demonstrate callback.\n",
    "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This example utilizes KerasNLP layers to train a sub-word vocabulary, tokenize training data, create a miniature GPT model, and perform inference with the text generation library.\n",
    "\n",
    "The trainded model is extremely small compared to the newest GPT models. If reportedly an dataset of 50Gb is needed to train a comperhensive language model for Bulgarian, then an model trained on 52 Mb dataset can be compared to an infant trying to speak its first words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "[Transformers for Bulgarian](https://acl-bg.org/proceedings/2023/RANLP%202023/pdf/2023.ranlp-1.77.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
