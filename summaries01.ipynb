{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Review in Knowledge Extraction from Knowledge Bases\n",
    "\n",
    "[Paper source](https://acl-bg.org/proceedings/2023/RANLP%202023/pdf/2023.ranlp-1.12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative language models achieve the state of the art in many tasks within natural language processing (NLP), but fail to interpret knowledge (semantics). The lack of interpretability of these models promotes the use of other technologies as a replacement or complement to generative language models for cases like research focused on incorporating knowledge by resorting to knowledge bases mainly in the form of graphs. The generation of large knowledge graphs is carried out with unsupervised or semi-supervised techniques, which promotes the validation of this knowledge with the same type of techniques due to the size of the generated databases.\n",
    "\n",
    "## Representing Knowledge - bases and graphs\n",
    "\n",
    "Knowledge bases (KB), generally represented by knowledge graphs (KG), storing sets of nodes and edges (relations between the nodes), are widely used for storage information used in different machine learning tasks. Traditional machine learning models, including deep neural networks use vectors as input, while the structure of KGs is more complex and can’t be simplified in a vector, due to the need for representing nodes, edges, connectivity, global relations inside the graph and features of every element. Methodologies used for extracting knowledge from KGs focus on creating latent vectors with the graph information (embeddings) or using neural networks specially designed for dealing with the graph structure.\n",
    "\n",
    "Many KBs are developed using non supervised machine learning techniques, generating massive data in the process. Those methods may cause errors when completing the KB due to false relations between nodes. Large KBs also have problems with not useful information introduced for a specific task which can be considered as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Knowledge\n",
    "\n",
    "For Entity Linking three different families of models are considered devided by to the techniques used to perform the task.\n",
    "\n",
    "### Translational Models\n",
    "\n",
    "These models, which consider that the different relationships between elements can be represented as displacements in space\n",
    "\n",
    "#### Euclidean Space Models\n",
    "\n",
    "Translational models express the existing relation between two entities as a translation in a vector space. Head entity h and tail entity t have a relation r which can translate the first entity to the second, this is the case for the first translational model - TransE. This model does not deal well with complex relations, i.e relations one-to-many (1-N), many-to-one (N-1) or many-to-many (N-N). TransH improves the representation of complex relations creating a unique hyperplane for each relation between two entities. The TransR considers both entities and relations should be in different spaces. This allow different entity representations according to the relation between them.\n",
    "\n",
    "The TransD model uses less parameters than its predecessor, this can be done using vector multiplications instead of matrices. It assumes two vectors for each entity and relation: the first vector (h, r, t) represents the meaning of the entity or relation and the second (hp, rp, tp) indicates how the entity must be proyected in the relation space, is utilized to map entities in the relation space. TransD uses the same number of parameters for each specific relation wich can lead to overfitting when using more parameters than necessary (simple relations) or underfitting when there are less parameters (complex relations). In TranSparse each relation uses a sparse matrix for each entity, with different sparse degrees. This enable the use of more or less parameters depending of the complexity of the relation.\n",
    "\n",
    "TransE regularization forces entity embeddings to stay inside a spherical vector space out of the range of the correct triple. The regularization used in TransE is normalization, making the magnitude of each embedding become 1 during each step of earning. This provoke a violation of equation making the sum of head entity and relation not equal to tail entity. This causes major problems, warping the embeddings obtained. To solve this TorusE creates entity and relation embeddings using the same principles as TransE but in a torus space.\n",
    "\n",
    "PairRE employs paired vectors for representing complex relations. These vectors proyect entities in the euclidean space where distance is minimized if the relation is right. The main advantage of PairRE is that both paired vectors allow more versatility in the loss function, achieving a better representation of complex relations.\n",
    "\n",
    "#### Complex Space Models\n",
    "\n",
    "Even if Euclidean space models progressively improve state of the art, they still have difficulties dealing with relations of symmetry, anti-symmetry, inversion and composition. \n",
    "\n",
    "RotatE tries to solve this problem with a complex space in order to represent embeddings using Euler’s identity. This way the translation from the head entity to the tail entity is a rotation. The model also changes the loss function introducing self adversarial samples, which improves the training process. The score function employed in RotatE is the the same as equation ($|h + r − t| ≈ 0 $) of TransE, but using Hadamard product instead of vector sum between head entity and relation. RotatE is improved with more dimension spaces through relation modeling with orthogonal transformations embeddings OTE. \n",
    "\n",
    "OTE makes orthogonal transformations with the head and relation vectors to the tail vector, and then from the tail and relation vectors to the head vector. Extending the idea of complex spaces, QuatE uses an hypercomplex space with 3 imaginary components i, j, k with the objective of having more degrees of freedom to the obtained embeddings. In this case, the scoring function utilized rotates head entity using the Hamilton product.\n",
    "\n",
    "#### Other Non-Euclidean Space Models\n",
    "\n",
    "Other models explore the posibility of using mathematical expresions out of the euclidean space.\n",
    "\n",
    "ManifoldE is a model that uses non-euclidean space. It considers that translational models are algebraically ill-conceived because they generate more equations than variables to solve, leading to approximate calculations for tasks like entity linking, where there are many entity candidates for one relation. In the case of ManifoldE, it uses a\n",
    "principle based on a ”manifold” function for expressing the relation between two entities. With this approach calculation should be exact, retrieving true candidates for each relation. ManifoldE expands the position of golden triples from one point (compared to TransE) to a manifold using a larger dimension sphere, diminishing noise when detecting true relations between all candidates and improving embedding vectors precision. Considering a head entity and a relation, all possible tail entities are inside a manifold of greater dimension (sphere). Scoring function is obtained as the difference in distance between radius of the sphere and the equation $|h + r − t| ≈ 0$. ManifoldE improves their results using a hyperplane as a manifold instead of a sphere.\n",
    "\n",
    "Hyperbolic space is ideal for modeling entities with hierarchical information due to its curvature. The problem with hyperbolic space is representing entities with different hierarchies under different relations. MuRP utilizes a Poincare Ball as a hyperbolic space, creating multi-relational embeddings for each entity and relation. The key of MuRP is using a hypersphere in hyperbolic space because it grows exponentially compare to euclidean space, having more space to separate each node. MuRP trains relation-specific parameters used for transforming entity embeddings through Mobius matrix-vector multiplication (in order to obtain the hyperbolic entity embeddings) and Mobius addition. The hyperbolic entity  embeddings are obtained by Mobius matrix-vector multiplication projecting the original embeddings to the tangent space of the Poincare ball transformed by the diagonal relation matrix and then projected back to Poincare ball.\n",
    "\n",
    "MuRP cannot encode some logical properties of relationships. It uses a fixed curvature for each relation. Although specific curvature for each relation would represent better hierarchies based on the context, it also uses only translations in the hyperbolic space. By contrast, ATTH creates embeddings in hyperbolic space using reflexions and\n",
    "rotations, enabling RotatE patterns to be captured, as well as considering a relation-specific curvature cr that allows a variety of hierarchies. Rotations are created with Givens transformations matrices due to this model does not employ complex numbers. ATTH use entity biases in the scoring function which act as margins for triples.\n",
    "\n",
    "Previous methods are designed for creating entity and relation representations in Euclidean, Hyperbolic or Hyperspherical space, but no one of them compare results in different spaces. The Geometry Interaction Knowledge Graph Embeddings (GIE) considers vectors in Euclidean (E), Hyperbolic (H) and Hyperspherical (S) spaces for head and tail entities and uses an attention mechanism over each vector in order to prioritize the space which represents better knowledge from the entity. Vectors in Hyperbolic and Hyperspherical space are logarithmically mapped to tangent space before applying attention and then features are extracted. GIE has an attention vector with a specific component for each different space both for head and tail entities inside a triple.\n",
    "\n",
    "### Tensor Factorization Models\n",
    "\n",
    "These matrix factorization models represent the relationships between entities as tensors and perform decomposition operations on the tensors to represent each entity and relationship. This approach has some advantages over translational models: 1) tensors can represent multiple relations of any order, you just need to increase tensor dimensionality, 2) previous knowledge from the problem structure is not necessary in order to infer knowledge from data.\n",
    "\n",
    "#### Euclidean Space Models\n",
    "\n",
    "RESCAL is the first tensor factorization model created to represent relations between entities. In this model, each matrix is constructed representing the relation between two entities, like a confusion matrix, and each matrix indicates a specific relation. The data is given as a $(n · n · m)$ tensor where $n$ is the number of entities and $m$ is the number of relations. \n",
    "\n",
    "RESCAL employs the following factorization over each slice of tensor Xk: $Xk ≈ ARkAT , for k = 1, ..., m$ \n",
    "Where $A$ is a $n x r$ matrix containing latentcomponent representation of entities an $Rk$ is an $r x r$ matrix that models the interactions between latent components for relation $k$. Matrix $Rk$ is asymmetric, which is useful for considering whether a latent component acts as a subject or object, given that each entity has a unique latent-component representation even if it is a subject or object in a relation. Matrices $A$ and $Rk$ are computed solving the following minimization problem. In order to reduce training parameters in RESCAL, DistMult uses a diagonal matrix Wr instead of an asymmetric relation matrix. This leads to a more expressive model than transE with the same number of parameters, being\n",
    "as scalable as previously mentioned models but less expressive than RESCAL.\n",
    "\n",
    "Holographic embeddings use vector circular correlation to represent entity embeddings. HolE creates holographic embeddings for represent pairs of entities. Correlation makes HolE efficient to compute and scalable to large datasets. This operation can be considered as a compression of the tensor product, in circular correlation each component is a sum of a fixed partition of pairwise interactions. HolE can store and retrieve information via circular convolution and circular correlation, respectively and it also learns the embeddings of the data.\n",
    "\n",
    "SimplE is a tensor factorization method based con Canonical Polyadic(CP) decomposition. It uses two vectors for each entity (he, te) and relation (vr, vr−1 ). SimplE uses a similarity function for each triple which is the average of the CP scores for the current triple and its inverse relation triple.\n",
    "\n",
    "TuckER is a lineal model for tensor factorization which generalizes previous tensor factorization models like RESCAL, DistMult, ComplEx and SimplE based on Tucker decomposition. It makes a decomposition from the binary tensor of triplets. It factorizes a tensor into a core smaller tensor multiplying one matrix for each dimension in the original tensor. In the case of TuckER, the decomposition creates a smaller tensor $W$, and matrices $eh$, $wr$ and $et$ for head entity, relation and tail entity, respectively.\n",
    "\n",
    "#### Other Non-Euclidean Space Models\n",
    "\n",
    "As RotatE, ComplEx uses imaginary numbers in the complex space, in this case it performs tensor factorization using Hermitian dot product, which involves the conjugate-transform on one of the two vectors multiplied. With this type of dot product, we obtain a non symmetric matrix being able to represent antisymmetric relations while maintaining linearity and low time complexity.\n",
    "\n",
    "### Deep Neural Models\n",
    "\n",
    "The deep neural models are used to obtain the main characteristics of each possible relationship and determine whether they are truthful or encode information from nearby entities. Graph neural networks can encode information about neighbours from each specific node, introducing context during processing in the neural network.\n",
    "\n",
    "#### Graph Convolutional Networks (GCNs)\n",
    "\n",
    "The first GCN introduced generates hidden states for each node processed taking into consideration each neighbour and relation. For each GCN layer, the processed node adds information from each neighbour equally. The context given by graphs improves many tasks when dealing with relational data, this is the case for R-GCN, an encoder that produces a hidden state for each node considering neighbours but also specific relations, in contrast with original GCN, being suitable for processing heterogeneous graphs.\n",
    "\n",
    "#### Graph Attention Networks (GATs)\n",
    "\n",
    "GCNs make convolutions considering equal importance among all edges in the processed graph, which may be a shallow approach for tasks where specific nodes and edges have more important information than others. In order to solve this issue, Graph Attention Networks are introduced. GATs make a convolution considering different weights for each edge connected to a specific node and can have multiple weights associated for each edge equal to the number of\n",
    "attention heads.\n",
    "\n",
    "A2N uses attention mechanism with specific queries in order to generate conditioned embeddings taking into account each query with the neighborhood of a source entity. A scalar attention score is generated for each neighbour and then their embeddings are aggregated generating a new source embeddings. Lastly, concatenate the new source embedding with the initial and projecting it to obtain the final source embeddings. In the original paper, DistMult is utilized as an attention scoring function as it allows the projection of neighbors in the same space as target entities.\n",
    "\n",
    "The use of non-Euclidean spaces has been extended to graph neural networks as in the case of M2GNN. Previous models using non-Euclidean spaces only considered homogeneous relations, so they lack expressiveness in this respect. M2GNN creates a non-constant heterogeneous curvature space using new parameters in the network called curvature coefficients. The proposed architecture also makes use of attention heads to improve the accuracy obtained.\n",
    "\n",
    "#### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs utilized broadly in computer vision have recently been used for entity linking. The main reason is that CNNs can solve entity linking tasks with far less parameters than previous mentioned models like DistMult. CNNs are also considered\n",
    "a very expressive way of representing entities and relations comparing to translational models, due to the number of features extracted with the CNN filters.\n",
    "\n",
    "ConvE is the first convolutional model achieving good results with entity linking tasks. It is simple, as it uses only one convolutional layer with 2D convolutions, a proyection layer to the embedding dimension and an inner product to make the entity linking prediction. The convolution is made by first concatenating the 2D vectors from the head entity and relation embeddings.\n",
    "\n",
    "ConvKB uses a convolutional layer with 3-column matrices, where each matrix is made of the concatenation of the triple vectors (eh, r, et). The features obtained after convolution are concatenated and score is obtained performing a multiplication with a weight vector $w$. Filters used for convolution in previous models are designed arbitrarily, which can lead to a poor performance.\n",
    "\n",
    "In order to solve this problem, HypER uses a hypernetwork for determining the right filter for each relation. A fully\n",
    "connected layer is used for obtaining embeddings representing head entity and relation, then the hypernetwork creates the filters of each relation embedding which will be utilized during convolution of entity embeddings. The hypernetwork proposed is a single fully connected layer. HypER uses a weight matrix that projects the results to another dimensional space in order to make the dot product between head entity and tail entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Both in the case of translational models and in tensor factorization, there is a tendency to represent increasingly complex spaces, to the point of combining different types of spaces into one (euclidean, hyperspherical and hyperbolic) or to represent increasingly complex vector spaces (complex space, quaternions, etc.). However, in some cases it is\n",
    "observed that the state of the art is surpassed without necessarily increasing the complexity of the space represented; this is the case of SimplE (which achieves results similar to ComplEx) or Tucker.\n",
    "\n",
    "Alternative spaces to the euclidean with positive or negative curvature tend to better represent some properties of entities with a smaller number of features, such as circular relations in hyperspherical spaces and hierarchies in hyperbolic spaces, allowing the creation of embeddings at a lower computational cost.\n",
    "\n",
    "In the case of deep neural models, tests have also been carried out with positive and negative curvature spaces. In these cases, curvature is a parameter to be trained within the network. The current state of the art is led by models that\n",
    "combine different vector spaces (GIE, M2GNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT-guided Semantics for Zero-shot Learning\n",
    "\n",
    "[Paper PDF source](https://arxiv.org/pdf/2310.11657v1.pdf)\n",
    "\n",
    "[Paper code repository](https://github.com/FHShubho/CGS-ZSL/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zero-shot Learning (ZSL) approach aims to classify unseen objects not observed in training. A more generic version\n",
    "named Generalized ZSL (GZSL) attempts to predict a class from seen and unseen classes together. Researchers have\n",
    "started exploring ZSL and GZSL with 2D image datasets. Later, considering the availability of depth-sensing cameras,\n",
    "exploring ZSL on 3D point cloud data got considerable attention. For both 2D and 3D cases, semantic descriptions of classes play a pivotal role in transferring knowledge from seen to unseen classes. Class semantics are designed to describe all objects with a common set of features or components, working as a bridge between seen and unseen worlds.\n",
    "Prior works show that addressing ZSL tasks in 3D has more challenges than its 2D counterpart. Therefore, improving class semantics may help to address some challenges.\n",
    "\n",
    "Class semantics can be obtained manually (attribute vectors) or automatically (word vectors). Attributes are identifiable features to describe a class that require laborious human annotation to obtain and are not readily available for\n",
    "many large-scale or 3D point cloud datasets. In contrast, automatic word vectors are the output (as vectors) of language models, given class names as input. These models are usually trained using billions of text corpus from Wikipedia, news articles, etc. Compared to attributes, the automatic extraction of word vectors makes them more realistic for real-world applications. However, embeddings from word vectors are noisier than manual attribute vectors resulting in poorer ZSL performance than attributes. This issue becomes more challenging, especially for ZSL on 3D point cloud objects, because of pre-trained models, poor quality features, dataset size, etc.\n",
    "\n",
    "Usually, word vectors are calculated using a single class name as input. However, many related words and definitions associated with that single class name are also necessary to improve the representativeness of the word vector. Considering related semantics can improve the semantic description of the given class. Again, such related semantics can be obtained by hard manual annotations, a costly process, or noisy web crawling annotation. \n",
    "\n",
    "To address this problem, recent ChatGPT can be a valuable source for describing a class with related semantics and attributes. It is both automatic and less noisy. herefore, given a class name, we ask ChatGPT to describe that class with a paragraph of text containing related semantics and attributes. Then, a word embedding of that paragraph can be extracted using a language model(word2vec). As last step, the word embeddings from class names and ChatGPT can be linearly combined to calculate an improved word vector. This approach does not need any prompt engineering, so it can be used in any existing ZSL models to increase accuracy. The new approach is tested on seven methods DEM, LATEM , SYNC, GDAN, f-CLSWGAN, TF-VAEGAN and CADA-VAE covering both 2D image and 3D point cloud datasets (ModelNet10, ModelNet40 and\n",
    "ScanObjectNN). This method is also applicable for both synthetic ModelNet40 and real-world scanned ScanObjectNN cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) have gained significant attention due to numerous advantages across various applications. To harness this power, the ChatGPT model is utilized to generate supplementary descriptions for the class names during the training phase of ZSL. This description includes attributes and semantics related to a class that can enhance its discriminative ability from other classes. Encoding this description with a word vector by forwarding it through a language model (word2vec) can augment additional supervision to ZSL models.\n",
    "\n",
    "#### Problem Formulation\n",
    "\n",
    "Let $x ∈ R^K$ represent the input data, corresponding to either an image or point cloud data. Two sets of class labels,\n",
    "$Y^s = {y^s_1, ..., y^s_s}$ for seen classes and $Y^u = {y^u_1, ..., y^u_u }$ for unseen classes, with the seen and unseen labels are disjoint. Additionally, $E^s = {ϕ(y^s_1), ..., ϕ(y^s_s )}$ and $E^u = {ϕ(y^u_1 ), ..., ϕ(y^u_u)}$ represent the sets of semantic feature embeddings obtained using the embedding function $ϕ$, where $ϕ(y) ∈ R^d$. To proceed, the set of $n^s$ seen samples is defined as $D^s = {(x^s_i , y^s_i , e^s_i )}^{n_s}_{i=1}$, where $x^s_i$ represents the $i^th$ instance from the seen set, with ground truth label $y^s_i ∈ Y^s$ and corresponding semantic vector $e^s_i = ϕ(y^s_i ) ∈ E^s$. Similarly, the set of $n_u$ unseen samples is defined as $D^u = {(x^u_i , y^u_i , e^u_i )}^{n_u}_{i=1}$, where $x^u_i$ represents the $i^th$ sample from the unseen set, with ground truth label $y^u_i ∈ Y^u$ and corresponding semantic vector $e^u_i = ϕ(y^u_i ) ∈ E^u$. The two main tasks addressed are: Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL).\n",
    "\n",
    "#### Preliminaries\n",
    "\n",
    "Two main types ZSL methods are embedding and generative. Embedding methods learn to map both the visual features of the input data and the semantic attributes to a common space. They can recognize unseen classes in this space by comparing their attribute representations with the input’s features. In contrast, generative methods synthesize samples that look like unseen classes by using a mix of labeled data from seen classes and auxiliary information, such as class attributes or textual descriptions. Overall, in any ZSL methodology, an input backbone transfers the input data into a meaningful feature embedding.\n",
    "\n",
    "Embedding-based ZSL aims to learn functions that map input data (images or point clouds) and semantic information (attributes or class labels) to a shared space. In this space, the model can link input features with semantic\n",
    "information, allowing it to identify and categorize classes it has not seen before. In general, there are two branches in\n",
    "the embedding approach. In the first, the input feature embedding is forwarded into a fully connected layer. Simultaneously, the corresponding semantic representation is fed into a fully connected layer, which maps the semantic embedding to a common space, where the Euclidean distance between the semantic and feature embeddings is minimized. This is achieved by optimizing the following objective function:\n",
    "\n",
    "$$L_E = \\frac1{n_s} \\sum^{n_s}_{i=1} ∥z^′_i − e^′_i∥^2_2 + λR(θ)$$\n",
    "\n",
    "where the $θ$ is the trainable parameters, and $R$ refers to the regularization loss function. The hyperparameter $λ$ is crucial in controlling the trade-off between the regularization and embedding losses.\n",
    "\n",
    "In generative Zero-Shot Learning (ZSL) methods, the objective is to generate synthetic samples for the unseen classes based on their semantic attributes. These methods enable the model to generate realistic and representative samples of unseen classes by leveraging the semantic information associated with those classes. The goal is to learn a conditional\n",
    "Generative Adversarial Network (GAN) model, denoted as $G$, which takes random Gaussian noise $h ∼ N (0, 1)$ and the\n",
    "semantic class embedding $e_i$ as inputs to generate the feature representation of class $i$, denoted as $\\hat{z}_i ∈ R^m$. Concurrently, a discriminator model is trained to classify real features against synthetic features. The objective function for generating synthetic feature samples is defined as follows:\n",
    "\n",
    "$$L_G =E_{z,e}[D(z, e)] − E_{z,e}[D(\\hat{z}, e)]− ηE_{\\tilde{z},eh}[(∥∇_{\\tilde{z}}D(\\tilde{z}, e)∥_2 − 1)^2]$$\n",
    "\n",
    "where $\\tilde{z} = βz + (1 − β)\\tilde{z}$, with $β ∼ U(0, 1)$, and $η$ is the penalty coefficient. This objective function guides the training of our conditional GAN model, enabling it to generate realistic and diverse feature representations based on the input class embeddings and Gaussian noise. Also, a classification loss is required to ensure that the synthetic samples are suitable for the classifier. This loss is defined as follows:\n",
    "\n",
    "$$L_C = −E_{\\hat{z}∼p_{\\hat{z}}} [log P (y | \\hat{z}; Θ)]$$\n",
    "\n",
    "The provided loss function is computed using a linear softmax classifier parameterized by $Θ$, which undergoes pretraining on the real features $z ∈ D^s$ from seen classes. To elaborate, this loss function serves as a regularizer, motivating the generator to construct discerning features in its generated samples.\n",
    "\n",
    "#### Improved class semantics\n",
    "\n",
    "The ChatGPT model is utilized by providing it with the class name of a seen class to generate descriptive sentences. Specifically the class names are given to the GPT-3.5, using prompts such as “Describe the [CLASS] in at most ten sentences, focusing on specific physical features and excluding any unavailable features.” The generated sentences\n",
    "offer detailed descriptions of the class names, aiding in improving class semantics for our ZSL tasks.\n",
    "\n",
    "A multi-step process is devised to enrich the understanding of class characteristics. The class name is initially processed by ChatGPT, generating a comprehensive description. This description includes multiple sentences closely related\n",
    "to the class name. Subsequently, both the class name’s semantic representation and the ChatGPT output’s semantic representation are extracted using a Word2vec model. These semantic representations offer valuable insights into the meaning and context of the class name and its associated description. Afterwards, these semantic descriptions are passed through separate fully connected layers before being merged. The merging process through addition allows the combination of the class-specific information with the contextual details from ChatGPT, resulting in a more comprehensive\n",
    "and enriched representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "ChatGPT-based word vectors fused with traditional class name-based vectors can achieve better ZSL and GZSL performance on 2D image and 3D point cloud recognition tasks. Experiments on multiple embedding-based and gener-ative ZSL methods show that this technique consistently improves those methods’ existing performance. ChatGPT could be a suitable annotation\n",
    "tool that can provide automatic and less noisy information without manual labor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Showcase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up ChatGPT API and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_KEY = 'inssert API key here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject list at 0x203eb9dfab0> JSON: {\n",
       "  \"object\": \"list\",\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"id\": \"curie-search-query\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172509,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"babbage-search-query\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172509,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"dall-e-3\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698785189,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"babbage-search-document\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172510,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"dall-e-2\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698798177,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-0301\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677649963,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"text-embedding-ada-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1671217299,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"davinci-search-query\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172505,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-hd-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699053533,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-hd\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699046015,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"ada-search-document\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172507,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"ada-code-search-code\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172505,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"davinci-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692634301,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"babbage-002\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692634615,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"davinci-search-document\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172509,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"curie-search-document\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172508,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"whisper-1\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677532384,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"ada-search-query\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172505,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"ada-code-search-text\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1651172510,\n",
       "      \"owned_by\": \"openai-dev\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1685474247,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-16k\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1683758102,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1677610602,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-0613\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1686587434,\n",
       "      \"owned_by\": \"openai\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1698959748,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1692901427,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1694122472,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1-1106\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1699053241,\n",
       "      \"owned_by\": \"system\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"tts-1\",\n",
       "      \"object\": \"model\",\n",
       "      \"created\": 1681940951,\n",
       "      \"owned_by\": \"openai-internal\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_key = GPT_KEY\n",
    "openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell me a joke\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ivo\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"tell me a joke\"}]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from [here](https://data.caltech.edu/records/65de6-vp158).\n",
    "\n",
    "Read the class names and generate description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file names from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Black_footed_Albatross',\n",
       " 1: 'Laysan_Albatross',\n",
       " 2: 'Sooty_Albatross',\n",
       " 3: 'Groove_billed_Ani',\n",
       " 4: 'Crested_Auklet',\n",
       " 5: 'Least_Auklet',\n",
       " 6: 'Parakeet_Auklet',\n",
       " 7: 'Rhinoceros_Auklet',\n",
       " 8: 'Brewer_Blackbird',\n",
       " 9: 'Red_winged_Blackbird',\n",
       " 10: 'Rusty_Blackbird',\n",
       " 11: 'Yellow_headed_Blackbird',\n",
       " 12: 'Bobolink',\n",
       " 13: 'Indigo_Bunting',\n",
       " 14: 'Lazuli_Bunting',\n",
       " 15: 'Painted_Bunting',\n",
       " 16: 'Cardinal',\n",
       " 17: 'Spotted_Catbird',\n",
       " 18: 'Gray_Catbird',\n",
       " 19: 'Yellow_breasted_Chat',\n",
       " 20: 'Eastern_Towhee',\n",
       " 21: 'Chuck_will_Widow',\n",
       " 22: 'Brandt_Cormorant',\n",
       " 23: 'Red_faced_Cormorant',\n",
       " 24: 'Pelagic_Cormorant',\n",
       " 25: 'Bronzed_Cowbird',\n",
       " 26: 'Shiny_Cowbird',\n",
       " 27: 'Brown_Creeper',\n",
       " 28: 'American_Crow',\n",
       " 29: 'Fish_Crow',\n",
       " 30: 'Black_billed_Cuckoo',\n",
       " 31: 'Mangrove_Cuckoo',\n",
       " 32: 'Yellow_billed_Cuckoo',\n",
       " 33: 'Gray_crowned_Rosy_Finch',\n",
       " 34: 'Purple_Finch',\n",
       " 35: 'Northern_Flicker',\n",
       " 36: 'Acadian_Flycatcher',\n",
       " 37: 'Great_Crested_Flycatcher',\n",
       " 38: 'Least_Flycatcher',\n",
       " 39: 'Olive_sided_Flycatcher',\n",
       " 40: 'Scissor_tailed_Flycatcher',\n",
       " 41: 'Vermilion_Flycatcher',\n",
       " 42: 'Yellow_bellied_Flycatcher',\n",
       " 43: 'Frigatebird',\n",
       " 44: 'Northern_Fulmar',\n",
       " 45: 'Gadwall',\n",
       " 46: 'American_Goldfinch',\n",
       " 47: 'European_Goldfinch',\n",
       " 48: 'Boat_tailed_Grackle',\n",
       " 49: 'Eared_Grebe',\n",
       " 50: 'Horned_Grebe',\n",
       " 51: 'Pied_billed_Grebe',\n",
       " 52: 'Western_Grebe',\n",
       " 53: 'Blue_Grosbeak',\n",
       " 54: 'Evening_Grosbeak',\n",
       " 55: 'Pine_Grosbeak',\n",
       " 56: 'Rose_breasted_Grosbeak',\n",
       " 57: 'Pigeon_Guillemot',\n",
       " 58: 'California_Gull',\n",
       " 59: 'Glaucous_winged_Gull',\n",
       " 60: 'Heermann_Gull',\n",
       " 61: 'Herring_Gull',\n",
       " 62: 'Ivory_Gull',\n",
       " 63: 'Ring_billed_Gull',\n",
       " 64: 'Slaty_backed_Gull',\n",
       " 65: 'Western_Gull',\n",
       " 66: 'Anna_Hummingbird',\n",
       " 67: 'Ruby_throated_Hummingbird',\n",
       " 68: 'Rufous_Hummingbird',\n",
       " 69: 'Green_Violetear',\n",
       " 70: 'Long_tailed_Jaeger',\n",
       " 71: 'Pomarine_Jaeger',\n",
       " 72: 'Blue_Jay',\n",
       " 73: 'Florida_Jay',\n",
       " 74: 'Green_Jay',\n",
       " 75: 'Dark_eyed_Junco',\n",
       " 76: 'Tropical_Kingbird',\n",
       " 77: 'Gray_Kingbird',\n",
       " 78: 'Belted_Kingfisher',\n",
       " 79: 'Green_Kingfisher',\n",
       " 80: 'Pied_Kingfisher',\n",
       " 81: 'Ringed_Kingfisher',\n",
       " 82: 'White_breasted_Kingfisher',\n",
       " 83: 'Red_legged_Kittiwake',\n",
       " 84: 'Horned_Lark',\n",
       " 85: 'Pacific_Loon',\n",
       " 86: 'Mallard',\n",
       " 87: 'Western_Meadowlark',\n",
       " 88: 'Hooded_Merganser',\n",
       " 89: 'Red_breasted_Merganser',\n",
       " 90: 'Mockingbird',\n",
       " 91: 'Nighthawk',\n",
       " 92: 'Clark_Nutcracker',\n",
       " 93: 'White_breasted_Nuthatch',\n",
       " 94: 'Baltimore_Oriole',\n",
       " 95: 'Hooded_Oriole',\n",
       " 96: 'Orchard_Oriole',\n",
       " 97: 'Scott_Oriole',\n",
       " 98: 'Ovenbird',\n",
       " 99: 'Brown_Pelican',\n",
       " 100: 'White_Pelican',\n",
       " 101: 'Western_Wood_Pewee',\n",
       " 102: 'Sayornis',\n",
       " 103: 'American_Pipit',\n",
       " 104: 'Whip_poor_Will',\n",
       " 105: 'Horned_Puffin',\n",
       " 106: 'Common_Raven',\n",
       " 107: 'White_necked_Raven',\n",
       " 108: 'American_Redstart',\n",
       " 109: 'Geococcyx',\n",
       " 110: 'Loggerhead_Shrike',\n",
       " 111: 'Great_Grey_Shrike',\n",
       " 112: 'Baird_Sparrow',\n",
       " 113: 'Black_throated_Sparrow',\n",
       " 114: 'Brewer_Sparrow',\n",
       " 115: 'Chipping_Sparrow',\n",
       " 116: 'Clay_colored_Sparrow',\n",
       " 117: 'House_Sparrow',\n",
       " 118: 'Field_Sparrow',\n",
       " 119: 'Fox_Sparrow',\n",
       " 120: 'Grasshopper_Sparrow',\n",
       " 121: 'Harris_Sparrow',\n",
       " 122: 'Henslow_Sparrow',\n",
       " 123: 'Le_Conte_Sparrow',\n",
       " 124: 'Lincoln_Sparrow',\n",
       " 125: 'Nelson_Sharp_tailed_Sparrow',\n",
       " 126: 'Savannah_Sparrow',\n",
       " 127: 'Seaside_Sparrow',\n",
       " 128: 'Song_Sparrow',\n",
       " 129: 'Tree_Sparrow',\n",
       " 130: 'Vesper_Sparrow',\n",
       " 131: 'White_crowned_Sparrow',\n",
       " 132: 'White_throated_Sparrow',\n",
       " 133: 'Cape_Glossy_Starling',\n",
       " 134: 'Bank_Swallow',\n",
       " 135: 'Barn_Swallow',\n",
       " 136: 'Cliff_Swallow',\n",
       " 137: 'Tree_Swallow',\n",
       " 138: 'Scarlet_Tanager',\n",
       " 139: 'Summer_Tanager',\n",
       " 140: 'Artic_Tern',\n",
       " 141: 'Black_Tern',\n",
       " 142: 'Caspian_Tern',\n",
       " 143: 'Common_Tern',\n",
       " 144: 'Elegant_Tern',\n",
       " 145: 'Forsters_Tern',\n",
       " 146: 'Least_Tern',\n",
       " 147: 'Green_tailed_Towhee',\n",
       " 148: 'Brown_Thrasher',\n",
       " 149: 'Sage_Thrasher',\n",
       " 150: 'Black_capped_Vireo',\n",
       " 151: 'Blue_headed_Vireo',\n",
       " 152: 'Philadelphia_Vireo',\n",
       " 153: 'Red_eyed_Vireo',\n",
       " 154: 'Warbling_Vireo',\n",
       " 155: 'White_eyed_Vireo',\n",
       " 156: 'Yellow_throated_Vireo',\n",
       " 157: 'Bay_breasted_Warbler',\n",
       " 158: 'Black_and_white_Warbler',\n",
       " 159: 'Black_throated_Blue_Warbler',\n",
       " 160: 'Blue_winged_Warbler',\n",
       " 161: 'Canada_Warbler',\n",
       " 162: 'Cape_May_Warbler',\n",
       " 163: 'Cerulean_Warbler',\n",
       " 164: 'Chestnut_sided_Warbler',\n",
       " 165: 'Golden_winged_Warbler',\n",
       " 166: 'Hooded_Warbler',\n",
       " 167: 'Kentucky_Warbler',\n",
       " 168: 'Magnolia_Warbler',\n",
       " 169: 'Mourning_Warbler',\n",
       " 170: 'Myrtle_Warbler',\n",
       " 171: 'Nashville_Warbler',\n",
       " 172: 'Orange_crowned_Warbler',\n",
       " 173: 'Palm_Warbler',\n",
       " 174: 'Pine_Warbler',\n",
       " 175: 'Prairie_Warbler',\n",
       " 176: 'Prothonotary_Warbler',\n",
       " 177: 'Swainson_Warbler',\n",
       " 178: 'Tennessee_Warbler',\n",
       " 179: 'Wilson_Warbler',\n",
       " 180: 'Worm_eating_Warbler',\n",
       " 181: 'Yellow_Warbler',\n",
       " 182: 'Northern_Waterthrush',\n",
       " 183: 'Louisiana_Waterthrush',\n",
       " 184: 'Bohemian_Waxwing',\n",
       " 185: 'Cedar_Waxwing',\n",
       " 186: 'American_Three_toed_Woodpecker',\n",
       " 187: 'Pileated_Woodpecker',\n",
       " 188: 'Red_bellied_Woodpecker',\n",
       " 189: 'Red_cockaded_Woodpecker',\n",
       " 190: 'Red_headed_Woodpecker',\n",
       " 191: 'Downy_Woodpecker',\n",
       " 192: 'Bewick_Wren',\n",
       " 193: 'Cactus_Wren',\n",
       " 194: 'Carolina_Wren',\n",
       " 195: 'House_Wren',\n",
       " 196: 'Marsh_Wren',\n",
       " 197: 'Rock_Wren',\n",
       " 198: 'Winter_Wren',\n",
       " 199: 'Common_Yellowthroat'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds = os.listdir('CUB_200_2011\\images')\n",
    "\n",
    "bird_names = {}\n",
    "\n",
    "for i in birds:\n",
    "    index, name = i.split('.')[0], i.split('.')[1]\n",
    "    index = int(index) - 1\n",
    "    bird_names[index] = name\n",
    "\n",
    "bird_names = dict(sorted(bird_names.items(), key=lambda x:x[0]))\n",
    "bird_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear underscores from file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Black footed Albatross',\n",
       " 1: 'Laysan Albatross',\n",
       " 2: 'Sooty Albatross',\n",
       " 3: 'Groove billed Ani',\n",
       " 4: 'Crested Auklet',\n",
       " 5: 'Least Auklet',\n",
       " 6: 'Parakeet Auklet',\n",
       " 7: 'Rhinoceros Auklet',\n",
       " 8: 'Brewer Blackbird',\n",
       " 9: 'Red winged Blackbird',\n",
       " 10: 'Rusty Blackbird',\n",
       " 11: 'Yellow headed Blackbird',\n",
       " 12: 'Bobolink',\n",
       " 13: 'Indigo Bunting',\n",
       " 14: 'Lazuli Bunting',\n",
       " 15: 'Painted Bunting',\n",
       " 16: 'Cardinal',\n",
       " 17: 'Spotted Catbird',\n",
       " 18: 'Gray Catbird',\n",
       " 19: 'Yellow breasted Chat',\n",
       " 20: 'Eastern Towhee',\n",
       " 21: 'Chuck will Widow',\n",
       " 22: 'Brandt Cormorant',\n",
       " 23: 'Red faced Cormorant',\n",
       " 24: 'Pelagic Cormorant',\n",
       " 25: 'Bronzed Cowbird',\n",
       " 26: 'Shiny Cowbird',\n",
       " 27: 'Brown Creeper',\n",
       " 28: 'American Crow',\n",
       " 29: 'Fish Crow',\n",
       " 30: 'Black billed Cuckoo',\n",
       " 31: 'Mangrove Cuckoo',\n",
       " 32: 'Yellow billed Cuckoo',\n",
       " 33: 'Gray crowned Rosy Finch',\n",
       " 34: 'Purple Finch',\n",
       " 35: 'Northern Flicker',\n",
       " 36: 'Acadian Flycatcher',\n",
       " 37: 'Great Crested Flycatcher',\n",
       " 38: 'Least Flycatcher',\n",
       " 39: 'Olive sided Flycatcher',\n",
       " 40: 'Scissor tailed Flycatcher',\n",
       " 41: 'Vermilion Flycatcher',\n",
       " 42: 'Yellow bellied Flycatcher',\n",
       " 43: 'Frigatebird',\n",
       " 44: 'Northern Fulmar',\n",
       " 45: 'Gadwall',\n",
       " 46: 'American Goldfinch',\n",
       " 47: 'European Goldfinch',\n",
       " 48: 'Boat tailed Grackle',\n",
       " 49: 'Eared Grebe',\n",
       " 50: 'Horned Grebe',\n",
       " 51: 'Pied billed Grebe',\n",
       " 52: 'Western Grebe',\n",
       " 53: 'Blue Grosbeak',\n",
       " 54: 'Evening Grosbeak',\n",
       " 55: 'Pine Grosbeak',\n",
       " 56: 'Rose breasted Grosbeak',\n",
       " 57: 'Pigeon Guillemot',\n",
       " 58: 'California Gull',\n",
       " 59: 'Glaucous winged Gull',\n",
       " 60: 'Heermann Gull',\n",
       " 61: 'Herring Gull',\n",
       " 62: 'Ivory Gull',\n",
       " 63: 'Ring billed Gull',\n",
       " 64: 'Slaty backed Gull',\n",
       " 65: 'Western Gull',\n",
       " 66: 'Anna Hummingbird',\n",
       " 67: 'Ruby throated Hummingbird',\n",
       " 68: 'Rufous Hummingbird',\n",
       " 69: 'Green Violetear',\n",
       " 70: 'Long tailed Jaeger',\n",
       " 71: 'Pomarine Jaeger',\n",
       " 72: 'Blue Jay',\n",
       " 73: 'Florida Jay',\n",
       " 74: 'Green Jay',\n",
       " 75: 'Dark eyed Junco',\n",
       " 76: 'Tropical Kingbird',\n",
       " 77: 'Gray Kingbird',\n",
       " 78: 'Belted Kingfisher',\n",
       " 79: 'Green Kingfisher',\n",
       " 80: 'Pied Kingfisher',\n",
       " 81: 'Ringed Kingfisher',\n",
       " 82: 'White breasted Kingfisher',\n",
       " 83: 'Red legged Kittiwake',\n",
       " 84: 'Horned Lark',\n",
       " 85: 'Pacific Loon',\n",
       " 86: 'Mallard',\n",
       " 87: 'Western Meadowlark',\n",
       " 88: 'Hooded Merganser',\n",
       " 89: 'Red breasted Merganser',\n",
       " 90: 'Mockingbird',\n",
       " 91: 'Nighthawk',\n",
       " 92: 'Clark Nutcracker',\n",
       " 93: 'White breasted Nuthatch',\n",
       " 94: 'Baltimore Oriole',\n",
       " 95: 'Hooded Oriole',\n",
       " 96: 'Orchard Oriole',\n",
       " 97: 'Scott Oriole',\n",
       " 98: 'Ovenbird',\n",
       " 99: 'Brown Pelican',\n",
       " 100: 'White Pelican',\n",
       " 101: 'Western Wood Pewee',\n",
       " 102: 'Sayornis',\n",
       " 103: 'American Pipit',\n",
       " 104: 'Whip poor Will',\n",
       " 105: 'Horned Puffin',\n",
       " 106: 'Common Raven',\n",
       " 107: 'White necked Raven',\n",
       " 108: 'American Redstart',\n",
       " 109: 'Geococcyx',\n",
       " 110: 'Loggerhead Shrike',\n",
       " 111: 'Great Grey Shrike',\n",
       " 112: 'Baird Sparrow',\n",
       " 113: 'Black throated Sparrow',\n",
       " 114: 'Brewer Sparrow',\n",
       " 115: 'Chipping Sparrow',\n",
       " 116: 'Clay colored Sparrow',\n",
       " 117: 'House Sparrow',\n",
       " 118: 'Field Sparrow',\n",
       " 119: 'Fox Sparrow',\n",
       " 120: 'Grasshopper Sparrow',\n",
       " 121: 'Harris Sparrow',\n",
       " 122: 'Henslow Sparrow',\n",
       " 123: 'Le Conte Sparrow',\n",
       " 124: 'Lincoln Sparrow',\n",
       " 125: 'Nelson Sharp tailed Sparrow',\n",
       " 126: 'Savannah Sparrow',\n",
       " 127: 'Seaside Sparrow',\n",
       " 128: 'Song Sparrow',\n",
       " 129: 'Tree Sparrow',\n",
       " 130: 'Vesper Sparrow',\n",
       " 131: 'White crowned Sparrow',\n",
       " 132: 'White throated Sparrow',\n",
       " 133: 'Cape Glossy Starling',\n",
       " 134: 'Bank Swallow',\n",
       " 135: 'Barn Swallow',\n",
       " 136: 'Cliff Swallow',\n",
       " 137: 'Tree Swallow',\n",
       " 138: 'Scarlet Tanager',\n",
       " 139: 'Summer Tanager',\n",
       " 140: 'Artic Tern',\n",
       " 141: 'Black Tern',\n",
       " 142: 'Caspian Tern',\n",
       " 143: 'Common Tern',\n",
       " 144: 'Elegant Tern',\n",
       " 145: 'Forsters Tern',\n",
       " 146: 'Least Tern',\n",
       " 147: 'Green tailed Towhee',\n",
       " 148: 'Brown Thrasher',\n",
       " 149: 'Sage Thrasher',\n",
       " 150: 'Black capped Vireo',\n",
       " 151: 'Blue headed Vireo',\n",
       " 152: 'Philadelphia Vireo',\n",
       " 153: 'Red eyed Vireo',\n",
       " 154: 'Warbling Vireo',\n",
       " 155: 'White eyed Vireo',\n",
       " 156: 'Yellow throated Vireo',\n",
       " 157: 'Bay breasted Warbler',\n",
       " 158: 'Black and white Warbler',\n",
       " 159: 'Black throated Blue Warbler',\n",
       " 160: 'Blue winged Warbler',\n",
       " 161: 'Canada Warbler',\n",
       " 162: 'Cape May Warbler',\n",
       " 163: 'Cerulean Warbler',\n",
       " 164: 'Chestnut sided Warbler',\n",
       " 165: 'Golden winged Warbler',\n",
       " 166: 'Hooded Warbler',\n",
       " 167: 'Kentucky Warbler',\n",
       " 168: 'Magnolia Warbler',\n",
       " 169: 'Mourning Warbler',\n",
       " 170: 'Myrtle Warbler',\n",
       " 171: 'Nashville Warbler',\n",
       " 172: 'Orange crowned Warbler',\n",
       " 173: 'Palm Warbler',\n",
       " 174: 'Pine Warbler',\n",
       " 175: 'Prairie Warbler',\n",
       " 176: 'Prothonotary Warbler',\n",
       " 177: 'Swainson Warbler',\n",
       " 178: 'Tennessee Warbler',\n",
       " 179: 'Wilson Warbler',\n",
       " 180: 'Worm eating Warbler',\n",
       " 181: 'Yellow Warbler',\n",
       " 182: 'Northern Waterthrush',\n",
       " 183: 'Louisiana Waterthrush',\n",
       " 184: 'Bohemian Waxwing',\n",
       " 185: 'Cedar Waxwing',\n",
       " 186: 'American Three toed Woodpecker',\n",
       " 187: 'Pileated Woodpecker',\n",
       " 188: 'Red bellied Woodpecker',\n",
       " 189: 'Red cockaded Woodpecker',\n",
       " 190: 'Red headed Woodpecker',\n",
       " 191: 'Downy Woodpecker',\n",
       " 192: 'Bewick Wren',\n",
       " 193: 'Cactus Wren',\n",
       " 194: 'Carolina Wren',\n",
       " 195: 'House Wren',\n",
       " 196: 'Marsh Wren',\n",
       " 197: 'Rock Wren',\n",
       " 198: 'Winter Wren',\n",
       " 199: 'Common Yellowthroat'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_names = {k: v.replace('_', ' ') for k, v in bird_names.items()}\n",
    "bird_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert filenames into classes in datsframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bird_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black footed Albatross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laysan Albatross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sooty Albatross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Groove billed Ani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crested Auklet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Bird_name\n",
       "0  Black footed Albatross\n",
       "1        Laysan Albatross\n",
       "2         Sooty Albatross\n",
       "3       Groove billed Ani\n",
       "4          Crested Auklet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_names_df = pd.DataFrame(bird_names.items(), columns=['index', 'Bird_name'])\n",
    "bird_names_df.drop(['index'], axis=1, inplace=True)\n",
    "bird_names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add decription column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bird_name</th>\n",
       "      <th>Short_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black footed Albatross</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laysan Albatross</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sooty Albatross</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Groove billed Ani</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crested Auklet</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Bird_name Short_Description\n",
       "0  Black footed Albatross                  \n",
       "1        Laysan Albatross                  \n",
       "2         Sooty Albatross                  \n",
       "3       Groove billed Ani                  \n",
       "4          Crested Auklet                  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_names_df['Short_Description'] = ''\n",
    "bird_names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_short_description(bird_name):\n",
    "    time.sleep(30) \n",
    "    for delay_secs in (2**x for x in range(0, 6)):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Describe the {bird_name} bird in at most 10 sentences using the specific physical features and do not need to mention the features that are not available in the bird. Also, do not use any numeric in descriptions, instead, use words.\"}]\n",
    "            )\n",
    "            print(response.choices[0].message['content'].partition('.')[0])\n",
    "            return response.choices[0].message['content']\n",
    "            break\n",
    "        \n",
    "        except openai.OpenAIError as e:\n",
    "            randomness_collision_avoidance = random.randint(0, 1000) / 1000.0\n",
    "            sleep_dur = delay_secs + randomness_collision_avoidance\n",
    "            print(f\"Error: {e}. Retrying in {round(sleep_dur, 2)} seconds.\")\n",
    "            time.sleep(sleep_dur)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it for 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8120793b85f7440586b4b6d3a9dfd395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 1.8 seconds.\n",
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 2.84 seconds.\n",
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 4.48 seconds.\n",
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 8.08 seconds.\n",
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 16.2 seconds.\n",
      "Error: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.. Retrying in 32.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(2)):\n",
    "    bird_names_df['Short_Description'][i] = generate_short_description(bird_names_df['Bird_name'][i])\n",
    "\n",
    "bird_names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate short description for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(bird_names_df['Bird_name']))):\n",
    "    if bird_names_df['Short_Description'][i] == '':\n",
    "        bird_names_df['Short_Description'][i] = generate_short_description(bird_names_df['Bird_name'][i])\n",
    "\n",
    "bird_names_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_names_df.to_excel('CUB_description.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
